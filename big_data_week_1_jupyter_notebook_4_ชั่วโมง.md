# รายวิชา: ข้อมูลขนาดใหญ่ (Big Data)
## สัปดาห์ที่ 1: บทนำ + Python พื้นฐานสำหรับ Data Science

> Notebook นี้ออกแบบให้ใช้ประกอบการเรียนการสอน โดยเน้นการสลับเนื้อหาเชิงแนวคิดกับโค้ดทันที เพื่อให้นักศึกษาเห็นภาพว่า *Big Data เริ่มจาก Python อย่างไร* และสามารถนำไปประยุกต์ใช้ได้จริง

---

## Part 0: การเตรียมสภาพแวดล้อม

การเตรียมสภาพแวดล้อม (Environment Setup) เป็นก้าวแรกที่สำคัญของนักข้อมูล การตรวจสอบเวอร์ชันของเครื่องมือช่วยลดปัญหาความเข้ากันได้ (Compatibility Issues) ที่อาจเกิดขึ้นเมื่อทำงานร่วมกับทีม หรือเมื่อนำโค้ดไปรันในเครื่องอื่น

```python
# ตรวจสอบเวอร์ชัน Python
import sys
print(f"Current Python Version: {sys.version}")
```

```python
# ตรวจสอบไลบรารีหลัก
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

---

## Part 1: แนะนำผู้สอน

### ผู้สอนรายวิชา
- ชื่อ–สกุล: ........................................
- บทบาท:
  - Data Engineer / Data Scientist
  - ผู้สอนด้าน Big Data และ Data Engineering
- ประสบการณ์ทำงานจริง:
  - ระบบข้อมูลขนาดใหญ่ (Cloud / Spark / Data Pipeline)
  - งานวิเคราะห์ข้อมูลในภาคธุรกิจ

**เป้าหมายของรายวิชา**
> สร้างความเข้าใจเชิงโครงสร้าง ไม่ใช่แค่การใช้เครื่องมือ

---

## Part 2: แนะนำรายวิชาและ Syllabus

### รายวิชานี้เรียนอะไรบ้าง
ในรายวิชานี้ เราจะเน้นสร้างความเข้าใจพื้นฐานที่แข็งแรง เพื่อให้นักศึกษาสามารถต่อยอดไปยังเครื่องมือระดับสูงได้
- Big Data คืออะไร (ในโลกจริง) และทำไมเราถึงต้องสนใจ
- Data Pipeline: ingest → clean → transform → analyze (หัวใจของการทำงานข้อมูล)
- เครื่องมือที่ใช้จริงในอุตสาหกรรม และแนวโน้มเทคโนโลยีล่าสุด

### รูปแบบการเรียน
- Lecture + Hands-on
- ใช้ Jupyter Notebook เป็นหลัก
- Final Project ใช้ข้อมูลขนาดใหญ่กว่า Excel

---

## Part 3: Big Data คืออะไร

> **วัตถุประสงค์ของช่วงนี้**: ให้นักศึกษาสามารถนิยาม Big Data ได้อย่างถูกต้องในเชิงระบบ แยกแยะจากคำศัพท์ที่เกี่ยวข้อง และเข้าใจ “เหตุผล” ที่ทำให้ต้องมีเครื่องมือ Big Data รวมถึงเห็นภาพว่าข้อมูลในโลกความเป็นจริงนั้นมีความซับซ้อนเพียงใด

---

### ปัญหาข้อมูลในโลกจริง (Reality of Data)
ในโลกจริง ข้อมูลไม่ได้ถูกสร้างเพื่อการวิเคราะห์ แต่เกิดจากกิจกรรมและระบบงาน เช่น การซื้อขาย การใช้งานแอป การสื่อสาร และอุปกรณ์ IoT

**ลักษณะของข้อมูลจริง**
- เกิดขึ้นตลอดเวลา และเพิ่มขึ้นอย่างต่อเนื่อง (Continuous Growth)
- มีหลายรูปแบบ (Structured, Semi-structured, Unstructured) เช่น ตัวเลข ข้อความ ภาพ เสียง พิกัด
- มีความผิดพลาด ขาดหาย ซ้ำซ้อน และมี noise (Dirty Data) ซึ่งต้องอาศัยการทำความสะอาดก่อนนำไปใช้

**ตัวอย่าง**
- ระบบเรียกรถ: ตำแหน่ง GPS ที่เปลี่ยนทุกไม่กี่วินาที + ข้อมูลการรับงาน + ราคา + รีวิว
- ระบบมหาวิทยาลัย: ข้อมูลลงทะเบียน + attendance + e-learning log + ข้อมูลกิจกรรม

**ช่วงถาม–คิด**
- ข้อมูล “จริง” แตกต่างจากข้อมูลในแบบฝึกหัดอย่างไร

---

### นิยาม Big Data (Definition as a System Constraint)
Big Data คือชุดข้อมูลหรือภาระงาน (workload) ที่มี **ขนาด ความเร็ว ความหลากหลาย หรือความไม่แน่นอน** สูงจนทำให้การจัดการด้วยเครื่องมือดั้งเดิมไม่เพียงพอ

**ประเด็นสำคัญ**
- Big Data ไม่ได้หมายถึง “ไฟล์ใหญ่” อย่างเดียว
- Big Data คือ **ข้อจำกัดของระบบเดิม** เมื่อเจอข้อมูลจริง

> กล่าวอีกนัยหนึ่ง: Big Data คือ “ปัญหา” มากกว่า “เทคโนโลยี”

---

### ข้อจำกัดของเครื่องมือดั้งเดิม (Why Excel / Single DB Fail)
เครื่องมือดั้งเดิมมักทำงานบนเครื่องเดียว (single machine) จึงติดข้อจำกัดเชิงฟิสิกส์

**ข้อจำกัดที่พบได้จริง**
เมื่อปริมาณข้อมูลเกินขีดจำกัดของ Hardware เครื่องเดียว เราจะเจอปัญหา:
- หน่วยความจำ (RAM) ไม่พอ → เปิดไฟล์ไม่ได้ Program Crash หรือทำงานช้ามาก
- CPU ไม่พอ → การคำนวณที่ซับซ้อนใช้เวลานานเกินไป (เช่น เป็นวันหรือสัปดาห์)
- I/O (อ่าน/เขียน) เป็นคอขวด → การอ่านไฟล์ขนาดใหญ่กินเวลานาน
- การทำงานพร้อมกันหลายคน (Concurrency) ทำได้ยากและเสี่ยงต่อข้อมูลเสียหาย

**ตัวอย่างสถานการณ์**
- ไฟล์ CSV ขนาด 5–20GB: เปิดไม่ได้ในเครื่องทั่วไป
- การ Join ข้อมูลหลายตารางขนาดใหญ่: ใช้เวลานานมาก

---

### 5Vs ของ Big Data (มองให้เป็น “ปัญหา”)
การใช้ 5Vs ไม่ใช่เพื่อท่องจำ แต่เพื่อ “จำแนกชนิดของปัญหา”

- **Volume**: ปริมาณข้อมูลมหาศาล จนเกินขีดจำกัดการเก็บและประมวลผลของเครื่องเดียว
- **Velocity**: ความเร็วของข้อมูลที่เข้ามาอย่างต่อเนื่อง เช่น ข้อมูลจาก Sensor หรือ Social Media ที่ต้องการการประมวลผลแบบ Real-time
- **Variety**: ความหลากหลายของรูปแบบข้อมูล ทั้ง Structured (SQL), Semi-structured (JSON, XML), และ Unstructured (Image, Video, Text)
- **Veracity**: ความถูกต้องและความน่าเชื่อถือของข้อมูล ซึ่งข้อมูล Big Data มักมีความไม่แน่นอนสูง
- **Value**: คุณค่าที่ซ่อนอยู่ ข้อมูลจะมีประโยชน์ก็ต่อเมื่อเราสามารถสกัด Insight ออกมาตัดสินใจได้จริง

**ช่วงถาม–คิด**
- ปัญหาที่ทำให้ระบบล่มบ่อยที่สุดอยู่ใน V ใด และเพราะอะไร

---

### Big Data vs Traditional Data (เปรียบเทียบเชิงสถาปัตยกรรม)
| มิติ | Traditional Data | Big Data |
|---|---|---|
| ขนาด | MB–GB | TB–PB หรือมากกว่า |
| การประมวลผล | Batch รายงานย้อนหลัง | Batch + Streaming + Interactive |
| สถาปัตยกรรม | Single-node | Distributed / Cluster |
| การขยาย | Vertical scaling | Horizontal scaling |
| รูปแบบข้อมูล | ส่วนใหญ่ Structured | Structured + Semi + Unstructured |

**สรุป**
Big Data ไม่ได้เปลี่ยน “เป้าหมาย” ของการวิเคราะห์ แต่เปลี่ยน “วิธี” ที่ต้องทำเพื่อให้ระบบรองรับได้

---

### ตัวอย่างเชิงกรณีศึกษา (E-commerce / Platform)
สมมติแพลตฟอร์มมี:
- ผู้ใช้งาน 500,000 คน/วัน
- Event log เฉลี่ย 50 events/คน/วัน → 25,000,000 events/วัน
- ข้อมูลคำสั่งซื้อ 200,000 รายการ/วัน
- ข้อมูลข้อความรีวิวและรูปภาพสินค้า

**คำถามที่องค์กรต้องตอบ**
- รายได้วันนี้เทียบกับสัปดาห์ก่อนเป็นอย่างไร
- สินค้าชนิดใดถูกคืนสูงผิดปกติ
- แคมเปญโฆษณาทำให้ conversion ดีขึ้นจริงหรือไม่

**ช่วงถาม–คิด**
- หากต้องได้คำตอบภายใน 5 นาที ระบบควรออกแบบแบบใด

---

### Big Data กับบทบาทในองค์กร (Why it matters)
Big Data สำคัญเพราะช่วยให้เกิดการตัดสินใจที่:
- เร็วขึ้น (timely)
- แม่นยำขึ้น (better evidence)
- รองรับสถานการณ์ซับซ้อน (complexity)

**ตัวอย่างผลลัพธ์ที่จับต้องได้**
- ลดต้นทุนการปฏิบัติการ (optimize logistics)
- เพิ่มรายได้ (recommendation / targeting)
- ลดความเสี่ยง (fraud detection)

---

### Big Data ไม่เท่ากับ Machine Learning (Clarify the Concepts)
**Big Data** คือระบบจัดเก็บ/ประมวลผล/จัดการข้อมูลให้พร้อมใช้งาน
**Machine Learning** คือวิธีใช้ข้อมูลเพื่อทำนายหรือจัดหมวดหมู่

ความสัมพันธ์:
- ถ้าข้อมูลไม่พร้อม (คุณภาพต่ำ/เข้าถึงยาก) → ML มักล้มเหลว
- หากมี Big Data ที่ดี → ML ทำงานได้ดีขึ้นและทำได้เร็วขึ้น

**ช่วงถาม–คิด**
- โครงการ ML ล้มเหลวได้เพราะ “ข้อมูล” ในมิติใดบ้าง

---

### ทำไมต้องมี Data Pipeline (From Raw to Usable)
ข้อมูลดิบ (Raw data) ไม่สามารถใช้วิเคราะห์ได้ทันที ต้องผ่านกระบวนการ

**Pipeline ขั้นพื้นฐาน**
1) **Ingest**: นำเข้าข้อมูลจากแหล่งต่างๆ (Database, API, Logs)
2) **Clean**: ทำความสะอาดข้อมูล จัดการค่าที่หายไป (Null) หรือค่าที่ผิดปกติ (Outliers)
3) **Transform**: แปลงรูปแบบข้อมูลให้พร้อมใช้ เช่น เปลี่ยนรูปแบบวันที่, รวมตาราง
4) **Store**: จัดเก็บลงระบบที่เหมาะสม (Data Warehouse, Data Lake)
5) **Analyze**: วิเคราะห์หา Insight หรือนำไปสร้าง Model

**ตัวอย่างง่าย**
- CSV → แปลงเป็น Parquet → อ่านเร็วขึ้น → วิเคราะห์ได้ไวขึ้น

---

### สรุปนิยามที่ถูกต้อง (Key Takeaways)
- Big Data คือ “ปัญหาเชิงระบบ” ที่เกิดจากข้อจำกัดของเครื่องมือดั้งเดิม
- เทคโนโลยี Big Data เกิดขึ้นเพื่อแก้ปัญหา 5Vs
- เป้าหมายของรายวิชาคือให้ทำ Pipeline ได้จริงและเข้าใจโครงสร้างระบบ

---

## Part 3.1: พัฒนาการทางประวัติศาสตร์ของ Big Data

> **วัตถุประสงค์ของช่วงนี้**: ให้นักศึกษาเข้าใจว่า “ข้อมูล” มีอยู่ก่อนคำว่า Big Data มานานแล้ว โดยเริ่มจากฐานข้อมูลในองค์กร (Database) → ระบบธุรกิจ (ERP/SAP) → ข้อมูลดิจิทัลขนาดมหาศาล (web-scale) → NoSQL → Data Lake/Lakehouse

---

### File-based Systems (ยุคข้อมูลกระจัดกระจาย)
- องค์กรจำนวนมากเริ่มจากการเก็บข้อมูลเป็นไฟล์ (CSV, Excel, Text)
- ข้อดี: เริ่มง่าย ต้นทุนต่ำ
- ข้อจำกัด: ซ้ำซ้อนสูง, เวอร์ชันไม่ตรงกัน, เชื่อมโยงข้อมูลยาก, ควบคุมคุณภาพยาก

---

### ฐานข้อมูล (Database) เกิดขึ้นเพื่ออะไร
แนวคิดฐานข้อมูลเกิดขึ้นเพื่อแก้ปัญหาไฟล์กระจัดกระจาย โดยทำให้ข้อมูล
- มีศูนย์กลาง (Centralized) ทำให้บริหารจัดการง่าย
- มีมาตรฐานและตรวจสอบได้ (Data Integrity)
- รองรับผู้ใช้หลายคนพร้อมกัน (Concurrency Control)

องค์ประกอบสำคัญที่ทำให้ Database ต่างจากไฟล์:
- การกำหนดโครงสร้างข้อมูล (schema)
- ดัชนี (index) เพื่อค้นหาเร็ว
- การควบคุมสิทธิ์และการทำงานพร้อมกัน (concurrency)

---

### Relational Database (RDBMS) และ SQL
- จัดข้อมูลเป็นตาราง + ความสัมพันธ์ระหว่างตาราง
- ใช้ SQL ในการสืบค้นและสรุปข้อมูล
- จุดแข็ง: ความถูกต้อง (ACID), ความสัมพันธ์ชัดเจน, ใช้ได้ดีมากกับงานธุรกรรม (OLTP)

ข้อจำกัด:
- เมื่อข้อมูลโตมาก/ผู้ใช้มาก การขยายระบบมีต้นทุนสูง และมักติดปัญหา scale แบบเครื่องเดียว

---

### สิ่งที่ RDBMS ทำได้ดี และสิ่งที่เริ่มเป็นข้อจำกัด
**RDBMS ทำได้ดี**
- ข้อมูลธุรกรรม: การเงิน การสั่งซื้อ การลงทะเบียน
- ความถูกต้องและการบันทึกประวัติ

**ข้อจำกัดที่เริ่มชัดเมื่อข้อมูลโต**
- งานวิเคราะห์ที่ต้อง scan จำนวนมากทำให้ช้า
- ข้อมูลนอกตาราง (ข้อความ/ภาพ/log) เก็บและใช้งานยาก
- ขยายระบบแบบแนวนอนทำได้ยากกว่าแบบไฟล์/ระบบกระจาย

---

### ยุคข้อมูลธุรกิจ — ERP (Business Process Data)
ก่อนคำว่า Big Data จะเป็นที่นิยม องค์กรธุรกิจมี “ข้อมูลจำนวนมาก” อยู่แล้วจากระบบงานหลัก

- **ERP (Enterprise Resource Planning)** เก็บข้อมูลกระบวนการธุรกิจ เช่น บัญชี การเงิน จัดซื้อ คลังสินค้า การผลิต
- เป้าหมายหลัก: ทำให้ข้อมูล “ถูกต้อง” และ “เป็นมาตรฐานเดียว” เพื่อการควบคุมภายในและการตัดสินใจ

ข้อจำกัด:
- ออกแบบเพื่อ “งานปฏิบัติการ” (OLTP) มากกว่างานวิเคราะห์เชิงลึก
- รายงานหนัก ๆ ทำให้ระบบช้าหรือกระทบงานธุรกรรม

---

### ERP → SAP และการทำมาตรฐานข้อมูลองค์กร
- การเติบโตของแพลตฟอร์ม ERP ระดับองค์กร เช่น **SAP** ทำให้ข้อมูลธุรกิจถูกทำให้เป็นมาตรฐาน
- องค์กรเริ่มมีข้อมูลที่เชื่อมโยงข้ามหน่วยงานได้มากขึ้น (end-to-end process)

ข้อจำกัดที่เริ่มชัด:
- ต้องการรายงานและการวิเคราะห์ที่หลากหลายมากขึ้น
- งานวิเคราะห์บน OLTP ทำให้เกิดปัญหาคอขวดด้านประสิทธิภาพ

---

### OLTP vs OLAP (แยกชนิดงาน)
- **OLTP (Online Transaction Processing)**: งานธุรกรรม เน้นเร็วและถูกต้อง (insert/update สูง) เช่น ระบบธนาคาร
- **OLAP (Online Analytical Processing)**: งานวิเคราะห์ เน้นการสรุป/รวม/เจาะลึก (scan/aggregate สูง) เช่น ออกรายงานประจำปี

บทเรียนสำคัญ:
- ระบบเดียวทำทั้ง OLTP และ OLAP พร้อมกันมักทำให้ประสิทธิภาพตกและบริหารยาก

---

### Data Warehouse และ ETL (1995–2005)
- แนวคิดสำคัญ: แยกพื้นที่วิเคราะห์ออกจากระบบธุรกรรม
- ใช้ **ETL** ดึงข้อมูลจากหลายระบบ → ทำความสะอาด/มาตรฐาน → โหลดเข้า Warehouse
- รองรับ BI และรายงานย้อนหลัง

ข้อจำกัด:
- ยืดหยุ่นต่ำ เปลี่ยน schema ยาก
- ไม่เหมาะกับข้อมูลนอกตาราง (ข้อความ/ภาพ/ล็อก)
- ความหน่วงในการได้ข้อมูล (latency) สูง

---

### การระเบิดของข้อมูลยุคเว็บ (Web-scale)
- Web/App ทำให้เกิดข้อมูลชนิดใหม่จำนวนมหาศาล เช่น clickstream, search log, ad impression
- ข้อมูลเกิดเร็วและมากกว่าที่ warehouse แบบเดิมรองรับได้

ผลที่ตามมา:
- ต้องการระบบที่เก็บได้ “มหาศาล” และประมวลผลได้ “แบบกระจาย”

---

### แนวคิด Distributed Systems (พื้นฐานสำคัญ)
- แบ่งข้อมูลและงานออกเป็นหลายเครื่อง (horizontal scaling)
- ได้ throughput สูงขึ้น
- ต้องออกแบบให้รองรับความล้มเหลวของเครื่อง (fault tolerance)

---

### Google File System (GFS) — การเก็บข้อมูลแบบกระจาย
- แนวคิด file system แบบกระจายเพื่อเก็บข้อมูลปริมาณมากบนเครื่องราคาทั่วไป
- ออกแบบให้ “ล้มได้” แต่ระบบยังทำงานต่อได้

---

### MapReduce — การประมวลผลแบบแบ่งงาน
- Map: กระจายงานย่อยไปหลายเครื่อง
- Reduce: รวมผลลัพธ์กลับมาเป็นคำตอบ

ข้อดี:
- ประมวลผลข้อมูลใหญ่ได้ด้วย cluster

ข้อจำกัด:
- งานหลายขั้นตอนซับซ้อน
- ช้าเพราะต้องอ่าน/เขียนลง disk บ่อย

---

### Bigtable — แนวคิดฐานข้อมูลแบบกระจายระดับเว็บ
- รองรับข้อมูลขนาดใหญ่แบบ web-scale
- เน้นการ scale ในแนวนอนและ throughput
- สะท้อนแนวคิดว่า “ฐานข้อมูลสำหรับยุคเว็บ” ต้องต่างจาก RDBMS แบบเดิม

---

### Colossus — พัฒนาการของระบบจัดเก็บเมื่อข้อมูลโตขึ้น
- เมื่อปริมาณและชนิดงานโตขึ้น ระบบจัดเก็บต้องพัฒนาให้จัดการ metadata และการใช้งานหลากหลายได้ดีขึ้น
- สะท้อนบทเรียนว่า เมื่อข้อมูลโตเป็นหลายลำดับขนาด (order of magnitude) สถาปัตยกรรม storage ต้องเปลี่ยนตาม

---

### Hadoop Ecosystem (2005–2012)
- HDFS: แนวคิด storage แบบกระจาย
- MapReduce: batch processing บน cluster

บทบาททางประวัติศาสตร์:
- ทำให้แนวคิด Big Data เข้าถึงได้ในวงกว้างผ่าน open-source

ข้อจำกัด:
- Disk-based → latency สูง
- พัฒนาและดูแลซับซ้อน

---

### NoSQL — เมื่อ RDBMS ไม่ตอบโจทย์ข้อมูลยุคใหม่
- Key-value / Document / Columnar
- จุดแข็ง: scale ง่าย รองรับข้อมูลหลากหลายรูปแบบ และ schema ยืดหยุ่น
- จุดอ่อน: คุณสมบัติด้านความสอดคล้องของข้อมูล (consistency) และการ query บางรูปแบบอาจต่างจาก RDBMS (ขึ้นกับระบบ)

**บทเรียนเชิงสถาปัตยกรรม**
- บางครั้งระบบต้อง “ยอมแลก” บางคุณสมบัติเพื่อแลกกับการ scale และความยืดหยุ่น

---

### Apache Spark (2012+)
- In-memory processing ช่วยลดเวลาประมวลผลอย่างมีนัยสำคัญ (เร็วกว่า Hadoop MapReduce ถึง 100 เท่าในบาง tasks)
- API ระดับสูง (DataFrame, SQL) ทำให้พัฒนาได้เร็วและปลอดภัยขึ้น
- รองรับงานหลายแบบ: Batch, Streaming, Machine Learning, Graph Processing

---

### Cloud และแนวคิดแยก Storage กับ Compute
- Elasticity: เพิ่ม/ลดทรัพยากรได้ตามต้องการ
- Pay-as-you-go: จ่ายตามการใช้งานจริง
- แยกที่เก็บข้อมูล (object storage) ออกจากเครื่องประมวลผล (compute)

---

### Data Lake — เก็บข้อมูลดิบเพื่อความยืดหยุ่น
- เก็บข้อมูลดิบจำนวนมาก รองรับหลายรูปแบบ
- schema-on-read ทำให้ยืดหยุ่นต่อคำถามใหม่ ๆ

ข้อควรระวัง:
- หากไม่มี governance จะกลายเป็น data swamp

---

### Lakehouse และสรุปวิวัฒนาการ
- Lakehouse รวมข้อดีของ Data Lake + Warehouse (เช่นความถูกต้องแบบ ACID บนไฟล์)
- แนวโน้มปัจจุบัน: analytics + ML + streaming บนข้อมูลชุดเดียวที่บริหารจัดการได้ดี

**ช่วงถาม–คิด (ปิดท้าย)**
- หากคุณเป็นองค์กรขนาดกลาง คุณควรเริ่มจากแนวคิดใดยุคใดก่อน และเพราะเหตุใด

---

## Part 4: ทำไม Python ถึงเป็นภาษาหลักของ Big Data

### เหตุผลสำคัญ
- **อ่านง่าย (Readability)**: โครงสร้างภาษาที่ใกล้เคียงภาษาอังกฤษ ทำให้เรียนรู้ได้เร็ว
- **Abstraction สูง**: ซ่อนความซับซ้อนของ memory management ทำให้โฟกัสที่ logic ได้
- **Performance ดี**: เมื่อใช้ร่วมกับไลบรารีอย่าง NumPy หรือ Pandas ที่เขียนด้วย C/C++
- **Ecosystem**: มีเครื่องมือครบวงจรสำหรับ Data Science (Pandas, Scikit-learn, TensorFlow)
- **Portability**: ใช้งานได้ข้ามแพลตฟอร์ม (Windows, Mac, Linux, Cloud)

---

## Part 5: Python พื้นฐาน (Hands-on)

> **เหตุผลที่ต้องเรียนพื้นฐานให้เป็นระบบก่อน**
> งาน Big Data ในทางปฏิบัติไม่ได้เริ่มจากการกดปุ่มในเครื่องมือ แต่เริ่มจากการเขียน “กระบวนการ” ที่ทำซ้ำได้ เช่น การอ่านข้อมูล การทำความสะอาด การแปลงรูปแบบ และการวิเคราะห์

### ทำไมต้องมีพัฒนาการของแนวคิด (Concept Evolution) ใน Python
แนวคิดในภาษาโปรแกรมพัฒนามาจากความต้องการทำให้โค้ด:
1) **อ่านง่าย** (readability)
2) **ดูแลง่าย** (maintainability)
3) **ขยายได้** (scalability of code)
4) **ทำงานร่วมกันได้** (collaboration)

กล่าวโดยสรุป: เมื่อระบบโตขึ้น การเขียนโค้ดแบบ “คิดเป็นส่วน” (abstraction) และ “มีโครงสร้าง” จะสำคัญมากกว่าการจำคำสั่ง

### เชื่อมกับ Data Science Libraries
ในโลกข้อมูล Python ถูกใช้อย่างกว้างขวางเพราะมีไลบรารีที่แปลงงานยากให้เป็นคำสั่งระดับสูง เช่น
- **NumPy**: คำนวณเชิงตัวเลขและอาร์เรย์อย่างมีประสิทธิภาพ
- **Pandas**: จัดการข้อมูลเชิงตาราง (เหมือน spreadsheet แต่ทรงพลังและทำซ้ำได้)
- **Matplotlib**: สร้างกราฟเพื่อสื่อสารผลลัพธ์

> หลังจากพื้นฐานส่วนนี้ นักศึกษาจะพร้อมเข้าสู่การจัดการข้อมูลด้วย Pandas และขยายไปสู่เครื่องมือ Big Data ในสัปดาห์ถัดไป

---

### 5.1 Variable & Operation

**แนวคิด**
ตัวแปร (Variable) คือชื่อที่ใช้อ้างอิงถึงข้อมูลที่ถูกเก็บไว้ในหน่วยความจำ เพื่อให้โปรแกรมสามารถนำข้อมูลนั้นกลับมาใช้งานซ้ำได้ โดยไม่ต้องเขียนค่าซ้ำหลายครั้ง

การทำ Operation คือการ “แปลง/คำนวณ” ข้อมูล ซึ่งเป็นพื้นฐานของการประมวลผลข้อมูลในงานจริง เช่น การคำนวณ KPI การแปลงหน่วย และการสร้างตัวแปรใหม่จากข้อมูลดิบ

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

#### ตัวอย่าง 1: การกำหนดตัวแปรและคำนวณพื้นฐาน
```python
a = 10
b = 3

(a + b, a - b, a * b, a / b)
```

#### ตัวอย่าง 2: Operation ทางคณิตศาสตร์ (Arithmetic Operations)
```python
# +  บวก
# -  ลบ
# *  คูณ
# /  หาร (ได้ผลลัพธ์เป็น float)
# // หารปัดเศษลง (floor division)
# %  หารเอาเศษ (modulo)
# ** ยกกำลัง (power)

a = 17
b = 5

results = {
    "a + b": a + b,
    "a - b": a - b,
    "a * b": a * b,
    "a / b": a / b,
    "a // b": a // b,
    "a % b": a % b,
    "a ** b": a ** b,
}
results
```

#### ตัวอย่าง 3: การอัปเดตค่าตัวแปร (Augmented Assignment)
```python
x = 10
x += 3   # เท่ากับ x = x + 3
x *= 2   # เท่ากับ x = x * 2
x
```

#### ตัวอย่าง 4: ตัวดำเนินการเปรียบเทียบ (Comparison Operations)
```python
a = 10
b = 3

comparisons = {
    "a == b": a == b,
    "a != b": a != b,
    "a > b": a > b,
    "a >= b": a >= b,
    "a < b": a < b,
    "a <= b": a <= b,
}
comparisons
```

#### ตัวอย่าง 5: ตัวดำเนินการเชิงตรรกะ (Logical Operations)
```python
# and, or, not

age = 19
is_student = True

eligible = (age >= 18) and is_student
eligible
```

**ช่วงถาม–คิด**
- การใช้ `//` และ `%` มีประโยชน์ต่อการจัดกลุ่มข้อมูลหรือการทำ partition อย่างไร
- ในงานข้อมูลจริง “การเปรียบเทียบ” ถูกใช้บ่อยในขั้นตอนใดของ pipeline

---

### 5.2 Data Types

**แนวคิด**
ชนิดข้อมูล (Data Types) มีผลต่อการคำนวณ การจัดเก็บ และความหมายของข้อมูล เช่น การบวกเลขกับการต่อข้อความเป็นคนละความหมายกัน

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

```python
x = 10          # int
y = 3.14        # float
name = "Data"  # string
flag = True     # boolean

(type(x), type(y), type(name), type(flag))
```

**ช่วงถาม–คิด**
- หากอ่านข้อมูลจากไฟล์แล้วตัวเลขถูกตีความเป็น string จะเกิดผลกระทบต่อการวิเคราะห์อย่างไร

---

### 5.3 Python Reserved Words (Keywords)

**แนวคิด**
คำสงวน (Reserved Words หรือ Keywords) คือคำที่ภาษา Python “จองไว้” เพื่อใช้งานเป็นไวยากรณ์ของภาษา จึงไม่ควรนำไปตั้งเป็นชื่อตัวแปรหรือชื่อฟังก์ชัน

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

```python
import keyword
keyword.kwlist  # แสดงรายการคำสงวนของ Python
```

**ตัวอย่างข้อควรระวัง**
- ไม่ควรตั้งชื่อตัวแปรเป็น: `class`, `def`, `for`, `if`, `import`, `return` เป็นต้น

---

### 5.4 Data Structures

#### List
```python
numbers = [1, 2, 3, 4]
numbers.append(5)
numbers
```

#### Dictionary
```python
student = {
    "name": "Alice",
    "score": 85
}
student["score"]
```

---

### 5.4 Condition

**แนวคิด:**
โครงสร้างเงื่อนไข (Condition) ใช้กำหนดเส้นทางการทำงานของโปรแกรมตามสถานการณ์ที่แตกต่างกัน ซึ่งเป็นพื้นฐานของการสร้างตรรกะ (Logic)

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

```python
score = 72

if score >= 80:
    result = "A"
elif score >= 70:
    result = "B"
else:
    result = "C"

result
```

**ช่วงถาม–คิด**
- หากเพิ่มเกณฑ์คะแนนใหม่ ต้องปรับโค้ดส่วนใด
- ระบบให้เกรดขนาดใหญ่จะเขียนด้วย if-else อย่างเดียวเพียงพอหรือไม่

---

### 5.5 Loop
```python
for i in range(5):
    print(i)
```

```python
numbers = [1, 2, 3]
for n in numbers:
    print(n * 2)
```

---

### 5.6 Function
```python
def average(x, y):
    return (x + y) / 2

average(10, 20)
```

---

### 5.7 Class & Object

**แนวคิด**
เมื่อโปรแกรมมีความซับซ้อนมากขึ้น เรามักต้องจัดกลุ่ม “ข้อมูล” และ “พฤติกรรม” ให้อยู่ด้วยกันอย่างเป็นระบบ แนวคิดนี้เรียกว่า **Object-Oriented Programming (OOP)**

- **Class** คือแม่แบบ (blueprint) ที่กำหนดว่าออบเจ็กต์ควรมีข้อมูล (attributes) และความสามารถ (methods) อะไร
- **Object (Instance)** คือสิ่งที่ถูกสร้างขึ้นจาก class

เหตุผลที่ใช้ class/object:
- ทำให้โค้ดอ่านง่ายขึ้นเมื่อระบบใหญ่
- ลดความซ้ำซ้อน
- แยกความรับผิดชอบของแต่ละส่วน (encapsulation)

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

#### ตัวอย่าง 1: Class พื้นฐาน (Student)
```python
class Student:
    def __init__(self, name, score):
        self.name = name
        self.score = score

    def grade(self):
        return "Pass" if self.score >= 60 else "Fail"

s1 = Student("Bob", 75)
s2 = Student("Alice", 55)

(s1.name, s1.score, s1.grade()), (s2.name, s2.score, s2.grade())
```

#### ตัวอย่าง 2: เพิ่มพฤติกรรมและการปรับปรุงข้อมูล (Update)
```python
class Student:
    def __init__(self, name, score):
        self.name = name
        self.score = score

    def add_bonus(self, bonus):
        self.score += bonus

    def summary(self):
        return f"{self.name}: score={self.score}, status={self.grade()}"

    def grade(self):
        return "Pass" if self.score >= 60 else "Fail"

s = Student("Chris", 58)
s.summary()
```

```python
s.add_bonus(5)
s.summary()
```

**ช่วงถาม–คิด**
- หากระบบมี “นักศึกษา” หลายพันคน การใช้ class/object ช่วยให้จัดการข้อมูลและการคำนวณได้อย่างไร
- แนวคิดนี้เชื่อมกับการออกแบบระบบข้อมูล (เช่น pipeline object หรือ dataset object) ได้อย่างไร

---

### 5.8 Import Module
```python
import math
math.sqrt(16)
```

---

## Part 6: Python สำหรับ Data Science

### 6.1 NumPy

**แนวคิด**
NumPy เป็นไลบรารีพื้นฐานด้านการคำนวณเชิงตัวเลข (numerical computing) โดยออกแบบให้ทำงานกับข้อมูลแบบอาร์เรย์ (array) ได้รวดเร็ว และรองรับการคำนวณแบบเวกเตอร์ (vectorization) ซึ่งเป็นพื้นฐานของการประมวลผลข้อมูลจำนวนมาก

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

#### ตัวอย่าง 1: สร้างอาร์เรย์และสถิติเบื้องต้น
```python
import numpy as np
arr = np.array([1, 2, 3, 4])
(arr.mean(), arr.sum(), arr.min(), arr.max())
```

#### ตัวอย่าง 2: การคำนวณแบบเวกเตอร์ (ไม่ต้องใช้ loop)
```python
x = np.array([10, 20, 30, 40])
(x * 1.1)  # เพิ่ม 10% ทุกค่าในครั้งเดียว
```

#### ตัวอย่าง 3: สร้างข้อมูลจำลองเพื่อทดลอง (Synthetic Data)
```python
np.random.seed(42)
values = np.random.normal(loc=70, scale=10, size=20)
values
```

---

### 6.2 Pandas

**แนวคิด**
Pandas เป็นเครื่องมือจัดการข้อมูลเชิงตาราง (tabular data) ที่ทำงานแบบทำซ้ำได้ (reproducible) เหมาะสำหรับขั้นตอน ingest/clean/transform ใน data pipeline ขนาดย่อม

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

#### ตัวอย่าง 1: สร้าง DataFrame
```python
import pandas as pd

data = {
    "city": ["Bangkok", "Chiang Mai", "Phuket", "Bangkok"],
    "population_m": [10.7, 1.2, 0.4, 10.7],
    "visitors_k": [120, 35, 50, 130]
}

df = pd.DataFrame(data)
df
```

#### ตัวอย่าง 2: เลือกคอลัมน์/คำนวณคอลัมน์ใหม่
```python
# สร้างคอลัมน์ใหม่จากการคำนวณ

df["visitors_per_million"] = df["visitors_k"] / df["population_m"]
df
```

#### ตัวอย่าง 3: สรุปข้อมูลด้วย groupby (เหมือน pivot แบบทำซ้ำได้)
```python
summary = df.groupby("city").agg(
    avg_visitors_k=("visitors_k", "mean"),
    rows=("city", "count")
).reset_index()
summary
```

#### ตัวอย่าง 4: ตรวจข้อมูลหาย (missing) และทำความสะอาด
```python
# สร้างข้อมูลตัวอย่างที่มี missing

df2 = df.copy()
df2.loc[1, "visitors_k"] = None

df2.isna().sum()
```

```python
# เติมค่าที่หายด้วยค่าเฉลี่ย (ตัวอย่าง)

df2["visitors_k"] = df2["visitors_k"].fillna(df2["visitors_k"].mean())
df2
```

---

### 6.3 Matplotlib

**แนวคิด**
การทำ Visualization มีเป้าหมายเพื่อสื่อสารแนวโน้มและความสัมพันธ์ของข้อมูล ไม่ใช่เพื่อความสวยงามเพียงอย่างเดียว กราฟที่ดีช่วยให้ตรวจความผิดปกติ (anomaly) และอธิบาย insight ได้ชัดเจน

```python
# พื้นที่สำหรับทดลองเขียนโค้ด


```

#### ตัวอย่าง 1: Bar chart
```python
import matplotlib.pyplot as plt

plt.bar(df["city"], df["visitors_k"])
plt.title("Visitors by City")
plt.ylabel("Visitors (thousand)")
plt.show()
```

#### ตัวอย่าง 2: Line chart จากตารางสรุป
```python
plt.plot(summary["city"], summary["avg_visitors_k"], marker="o")
plt.title("Average Visitors by City")
plt.ylabel("Avg visitors (thousand)")
plt.show()
```

#### ตัวอย่าง 3: Histogram ดูการกระจายของข้อมูล
```python
plt.hist(values, bins=8)
plt.title("Distribution of Simulated Scores")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.show()
```

---

## Part 7: เชื่อมโยงไป Big Data

- Pandas คือจุดเริ่มต้น
- เมื่อข้อมูลใหญ่ขึ้น → Spark / Distributed System
- โค้ดและแนวคิดยังเหมือนเดิม

> **Big Data = Python + แนวคิดระบบ**

---

## Part 8: สรุปท้ายคาบ

### สิ่งที่นักศึกษาควรได้วันนี้
- เข้าใจภาพรวม Big Data
- เห็นบทบาทของ Python
- เขียน Python พื้นฐานได้

### เตรียมตัวสัปดาห์หน้า
- อ่านเรื่อง CSV vs Parquet
- ทดลองใช้ Pandas เพิ่มเติม

---

> End of Week 1 Notebook

